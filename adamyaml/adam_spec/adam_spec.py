import yaml
import re
from pathlib import Path
from dataclasses import dataclass, asdict
from copy import deepcopy
import logging
from .merge_yaml import merge_yaml
from .schema_validator import SchemaValidator, ValidationResult

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class Column:
    """ADaM column specification."""
    name: str
    type: str
    label: str | None = None
    core: str | None = None
    derivation: dict | None = None
    validation: dict | None = None
    drop: bool | None = None
    
    def __post_init__(self):
        if self.label is None:
            self.label = self.name
    
    def to_dict(self) -> dict:
        """Convert to dictionary, excluding None values and drop flag."""
        result = {}
        for key, value in asdict(self).items():
            if value is not None and key != 'drop':
                result[key] = value
        return result


class AdamSpec:
    """
    Load and validate ADaM specifications from YAML files with inheritance support.
    
    Attributes:
        domain: Dataset domain (e.g., 'ADSL')
        key: List of key variables
        columns: List of Column objects
        parents: List of parent YAML files
    """
    
    def __init__(self, path: str | Path, schema_path: str | Path | None = None):
        """
        Load YAML specification with automatic parent merging and validation.
        
        Args:
            path: Path to YAML specification file
            schema_path: Optional schema for validation (defaults to spec's schema field)
        """
        self.path = Path(path)
        self.schema_path = Path(schema_path) if schema_path else None
        self.domain: str = ""  
        self.key: list[str] = []
        self.columns: list[Column] = []
        self.parents: list[str] = []
        self._errors: list[str] = []
        self._warnings: list[str] = []
        self._raw_spec: dict = {}
        self._schema_results: list[ValidationResult] = []
        
        self._build_spec()
        
        # Use schema from spec if not provided
        if not self.schema_path and 'schema' in self._raw_spec:
            schema_from_spec = self._raw_spec['schema']
            potential_schema_path = self.path.parent / schema_from_spec
            if potential_schema_path.exists():
                self.schema_path = potential_schema_path
                logger.info(f"Using schema from specification: {schema_from_spec}")
        
        # Validate if schema available
        if self.schema_path:
            self._validate_with_schema()
            if self._errors:
                logger.warning(f"Validation found {len(self._errors)} errors")
        else:
            logger.warning(f"No schema found for {self.path.name} - validation skipped")
    
    def _build_spec(self) -> None:
        """Build specification with inheritance."""
        if not self.path.exists():
            raise FileNotFoundError(f"YAML file not found: {self.path}")
        
        try:
            with open(self.path, 'r') as f:
                study_spec = yaml.safe_load(f) or {}
        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML in {self.path}: {e}")
        
        yaml_files = self._collect_yaml_files(study_spec)
        
        final_spec = merge_yaml(
            yaml_files,
            list_merge_strategy="merge_by_key",
            list_merge_keys={"columns": "name"}
        )
        
        self._raw_spec = final_spec
        self._extract_fields(final_spec)
    
    def _collect_yaml_files(self, study_spec: dict) -> list[str]:
        """Collect YAML files including parents."""
        yaml_files = []
        spec_dir = self.path.parent
        
        if 'parents' in study_spec:
            self.parents = study_spec['parents'] if isinstance(study_spec['parents'], list) else [study_spec['parents']]
            
            for parent_file in self.parents:
                parent_path = spec_dir / parent_file
                if not parent_path.exists():
                    raise FileNotFoundError(f"Parent file not found: {parent_path}")
                yaml_files.append(str(parent_path))
        
        yaml_files.append(str(self.path))  # Study file last to override
        return yaml_files
    
    def _extract_fields(self, spec: dict) -> None:
        """Extract fields from merged specification."""
        self.domain = spec.get('domain', '')
        self.key = spec.get('key', [])
        
        raw_columns = spec.get('columns', [])
        processed_columns = self._process_columns(raw_columns)
        
        self.columns = []
        for col_dict in processed_columns:
            try:
                if 'type' not in col_dict:
                    self._errors.append(f"Column {col_dict.get('name', 'unknown')} missing required 'type' field")
                    continue
                    
                col = Column(**{k: v for k, v in col_dict.items() if k != 'drop'})
                self.columns.append(col)
            except TypeError as e:
                self._errors.append(f"Invalid column specification: {e}")
    
    def _process_columns(self, columns: list[dict]) -> list[dict]:
        """Process columns, handling drop flags."""
        result = []
        dropped_names = set()
        
        for col in columns:
            if col.get('drop', False):
                dropped_names.add(col['name'])
                logger.debug(f"Column marked for drop: {col['name']}")
        
        for col in columns:
            if col.get('name') not in dropped_names and not col.get('drop', False):
                if 'label' not in col or col['label'] is None:
                    col['label'] = col['name']
                result.append(col)
        
        return result
    
    def _validate_with_schema(self) -> None:
        """Validate specification against schema."""
        try:
            validator = SchemaValidator(self.schema_path)
            self._schema_results = validator.validate(self._raw_spec)
            
            for error in validator.get_errors():
                self._errors.append(f"[{error.rule}] {error.message}")
            
            for warning in validator.get_warnings():
                self._warnings.append(f"[{warning.rule}] {warning.message}")
            
            logger.info(f"Schema validation complete: {len(validator.get_errors())} errors, {len(validator.get_warnings())} warnings")
        except Exception as e:
            logger.error(f"Schema validation failed: {e}")
            self._errors.append(f"Schema validation error: {e}")

    def to_dict(self, include_parents: bool = False) -> dict:
        """Convert to dictionary format."""
        result = deepcopy(self._raw_spec)
        
        result["domain"] = self.domain
        result["key"] = self.key
        result["columns"] = [col.to_dict() for col in self.columns]
        
        if not include_parents:
            result.pop('parents', None)
        
        return result
    
    def to_yaml(self, include_parents: bool = False) -> str:
        """Convert to YAML string."""
        return yaml.dump(
            self.to_dict(include_parents),
            default_flow_style=False,
            sort_keys=False
        )
    
    def save(self, output_path: str | Path) -> None:
        """Save specification to YAML file."""
        output = Path(output_path)
        
        with open(output, 'w') as f:
            f.write(self.to_yaml())
        
        logger.info(f"Saved YAML specification to {output}")
    
    def get_column_specs(self, names: str | list[str] | None = None) -> dict | list[dict] | None:
        """
        Get column specifications.
        
        Args:
            names: Column name(s) - string, list, or None for all
        
        Returns:
            Single dict, list of dicts, or None if not found
        """
        if names is not None:
            if isinstance(names, str):
                for col in self.columns:
                    if col.name == names:
                        return col.to_dict()
                return None
            
            elif isinstance(names, list):
                result = []
                for name in names:
                    for col in self.columns:
                        if col.name == name:
                            result.append(col.to_dict())
                            break
                return result
        
        return [col.to_dict() for col in self.columns]
    
    def get_data_dependency(self) -> list[dict]:
        """
        Extract SDTM data dependencies.
        
        Returns:
            List of dicts with adam_variable, sdtm_data, sdtm_variable
        """
        pattern = re.compile(r'\b([A-Z][A-Z0-9_]{0,19})\.([A-Z][A-Z0-9_]{0,19})\b')
        
        dependencies = []
        seen = set()
        
        for column in self.columns:
            matches = pattern.findall(str(column.to_dict()))
            
            for sdtm_data, sdtm_variable in matches:
                key = (column.name, sdtm_data, sdtm_variable)
                if key not in seen:
                    seen.add(key)
                    dependencies.append({
                        'adam_variable': column.name,
                        'sdtm_data': sdtm_data,
                        'sdtm_variable': sdtm_variable
                    })
        
        return dependencies
    
    @property
    def sdtm_dir(self) -> str:
        """
        Get the SDTM directory path from specification.
        
        Returns:
            Absolute path to SDTM directory
            
        Raises:
            ValueError: If sdtm_dir is not specified in the specification
        """
        sdtm_dir = self._raw_spec.get('sdtm_dir')
        
        if not sdtm_dir:
            raise ValueError(f"No sdtm_dir specified in {self.path}. This is a required field.")
        
        # Convert to Path for proper handling
        sdtm_path = Path(sdtm_dir)
        
        # If relative path, resolve relative to spec file directory
        if not sdtm_path.is_absolute():
            sdtm_path = self.path.parent / sdtm_path
            
        return str(sdtm_path.resolve())